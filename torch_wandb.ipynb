{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3ufHl4sr6tQwIIvA1IBUL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molabokchi/bokchi_open_lab/blob/main/torch_wandb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qq\n",
        "\n",
        "# Fetch audio, video and other data files to log\n",
        "!git clone https://github.com/wandb/examples.git\n",
        "!pip install soundfile -qq\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsH12RVDMqem",
        "outputId": "c2657f57-f28c-4927-b9e5-378d44724466"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m1.6/2.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'examples'...\n",
            "remote: Enumerating objects: 6689, done.\u001b[K\n",
            "remote: Counting objects: 100% (287/287), done.\u001b[K\n",
            "remote: Compressing objects: 100% (155/155), done.\u001b[K\n",
            "remote: Total 6689 (delta 149), reused 250 (delta 126), pack-reused 6402\u001b[K\n",
            "Receiving objects: 100% (6689/6689), 165.73 MiB | 17.73 MiB/s, done.\n",
            "Resolving deltas: 100% (3658/3658), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "eyNW5DtdMvgt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-AY-EZpkMjsx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # noqa: N812\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment init & config hyperparam\n",
        "wandb.init(project= \"torch_test\", \n",
        "                config={ 'learning_rate':0.001,'context_size':2 , 'embedding_dim':10})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "sq-L9YLZNG-N",
        "outputId": "a3fff2c7-4dfd-498f-99dd-ceb4b1cdbbc7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230201_090035-26gir5b8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/mola-team/torch_test/runs/26gir5b8\" target=\"_blank\">dazzling-noodles-1</a></strong> to <a href=\"https://wandb.ai/mola-team/torch_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href=\"https://wandb.ai/mola-team/torch_test\" target=\"_blank\">https://wandb.ai/mola-team/torch_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href=\"https://wandb.ai/mola-team/torch_test/runs/26gir5b8\" target=\"_blank\">https://wandb.ai/mola-team/torch_test/runs/26gir5b8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mola-team/torch_test/runs/26gir5b8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fcc21921d60>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test example\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)\n",
        "trigrams = [\n",
        "    ([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n",
        "    for i in range(len(test_sentence) - 2)\n",
        "]\n",
        "_hook_handles ={}\n",
        "\n",
        "\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "has_cuda = torch.cuda.is_available()\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), wandb.config.embedding_dim, wandb.config.context_size)\n",
        "model = model.cuda() if has_cuda else model\n",
        "optimizer = optim.SGD(model.parameters(), lr=wandb.config.learning_rate)\n",
        "wandb.watch(model, log_freq=1000)\n",
        "\n",
        "for i in range(100):\n",
        "    print(\"epoch :\" + str(i))\n",
        "    total_loss = 0\n",
        "    for batch_i, (context, target) in enumerate(trigrams):\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor(\n",
        "            [word_to_ix[w] for w in context], dtype=torch.long\n",
        "        )\n",
        "        context_idxs = context_idxs.cuda() if has_cuda else context_idxs\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        target = torch.tensor([word_to_ix[target]], dtype=torch.long)\n",
        "        target = target.cuda() if has_cuda else target\n",
        "        loss = loss_function(log_probs, target)\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "        if batch_i%10 ==0:\n",
        "          wandb.log({\"batch_loss\": loss.item()})\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # The loss decreased ev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlGaVvQfMpfQ",
        "outputId": "9cc24727-c1b3-4bb8-cdd5-72f22f81d78f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch :0\n",
            "epoch :1\n",
            "epoch :2\n",
            "epoch :3\n",
            "epoch :4\n",
            "epoch :5\n",
            "epoch :6\n",
            "epoch :7\n",
            "epoch :8\n",
            "epoch :9\n",
            "epoch :10\n",
            "epoch :11\n",
            "epoch :12\n",
            "epoch :13\n",
            "epoch :14\n",
            "epoch :15\n",
            "epoch :16\n",
            "epoch :17\n",
            "epoch :18\n",
            "epoch :19\n",
            "epoch :20\n",
            "epoch :21\n",
            "epoch :22\n",
            "epoch :23\n",
            "epoch :24\n",
            "epoch :25\n",
            "epoch :26\n",
            "epoch :27\n",
            "epoch :28\n",
            "epoch :29\n",
            "epoch :30\n",
            "epoch :31\n",
            "epoch :32\n",
            "epoch :33\n",
            "epoch :34\n",
            "epoch :35\n",
            "epoch :36\n",
            "epoch :37\n",
            "epoch :38\n",
            "epoch :39\n",
            "epoch :40\n",
            "epoch :41\n",
            "epoch :42\n",
            "epoch :43\n",
            "epoch :44\n",
            "epoch :45\n",
            "epoch :46\n",
            "epoch :47\n",
            "epoch :48\n",
            "epoch :49\n",
            "epoch :50\n",
            "epoch :51\n",
            "epoch :52\n",
            "epoch :53\n",
            "epoch :54\n",
            "epoch :55\n",
            "epoch :56\n",
            "epoch :57\n",
            "epoch :58\n",
            "epoch :59\n",
            "epoch :60\n",
            "epoch :61\n",
            "epoch :62\n",
            "epoch :63\n",
            "epoch :64\n",
            "epoch :65\n",
            "epoch :66\n",
            "epoch :67\n",
            "epoch :68\n",
            "epoch :69\n",
            "epoch :70\n",
            "epoch :71\n",
            "epoch :72\n",
            "epoch :73\n",
            "epoch :74\n",
            "epoch :75\n",
            "epoch :76\n",
            "epoch :77\n",
            "epoch :78\n",
            "epoch :79\n",
            "epoch :80\n",
            "epoch :81\n",
            "epoch :82\n",
            "epoch :83\n",
            "epoch :84\n",
            "epoch :85\n",
            "epoch :86\n",
            "epoch :87\n",
            "epoch :88\n",
            "epoch :89\n",
            "epoch :90\n",
            "epoch :91\n",
            "epoch :92\n",
            "epoch :93\n",
            "epoch :94\n",
            "epoch :95\n",
            "epoch :96\n",
            "epoch :97\n",
            "epoch :98\n",
            "epoch :99\n",
            "[519.8704817295074, 517.1198191642761, 514.3882398605347, 511.67625999450684, 508.9823508262634, 506.30453515052795, 503.64245104789734, 500.9934060573578, 498.3582260608673, 495.7370617389679, 493.1294605731964, 490.5339014530182, 487.9478950500488, 485.371399641037, 482.80445671081543, 480.24452805519104, 477.69172739982605, 475.1444516181946, 472.60357546806335, 470.0661771297455, 467.5324213504791, 465.00382375717163, 462.4772505760193, 459.9523162841797, 457.4264416694641, 454.9009487628937, 452.37582087516785, 449.85156512260437, 447.3260614871979, 444.79920506477356, 442.2708601951599, 439.73832631111145, 437.20281529426575, 434.66193294525146, 432.1170847415924, 429.5692081451416, 427.01488614082336, 424.45606446266174, 421.8903021812439, 419.31767654418945, 416.73732137680054, 414.14842200279236, 411.5512788295746, 408.94681692123413, 406.33262372016907, 403.70775640010834, 401.075292468071, 398.43365955352783, 395.7853696346283, 393.12637090682983, 390.4584010839462, 387.7796230316162, 385.0963913202286, 382.40353786945343, 379.70196282863617, 376.9897894859314, 374.2695908546448, 371.54324448108673, 368.80691730976105, 366.06197810173035, 363.30837881565094, 360.54538094997406, 357.77292943000793, 354.992355465889, 352.20220482349396, 349.40542125701904, 346.59972751140594, 343.7897869348526, 340.9711135625839, 338.1424344778061, 335.308017373085, 332.4692956209183, 329.62285327911377, 326.7738469839096, 323.91625809669495, 321.0536252260208, 318.1861513853073, 315.31264328956604, 312.43513721227646, 309.55425626039505, 306.67021065950394, 303.7818945646286, 300.89161014556885, 297.9995310306549, 295.1045610308647, 292.20983815193176, 289.31302803754807, 286.416834294796, 283.5202933549881, 280.6235565543175, 277.72776168584824, 274.8333235383034, 271.9424278140068, 269.0521467924118, 266.16478258371353, 263.28048288822174, 260.39973670244217, 257.5260212421417, 254.65498864650726, 251.79352569580078]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8lUNW4VM6rU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}