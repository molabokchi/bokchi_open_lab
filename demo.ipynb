{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molabokchi/bokchi_open_lab/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ![다운로드.png](data:image/png;base64,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) deepdriver quickstart!"
      ],
      "metadata": {
        "id": "ccBiuOu0rtin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "welecome to deepdriver! 😀"
      ],
      "metadata": {
        "id": "ZGV9WnWBrvb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can refer to the guide document.\n",
        "https://bokchi.gitbook.io/deepdriver-ce/"
      ],
      "metadata": {
        "id": "RuepMN3Yyuu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 💻 install deepdriver & requirement package for train"
      ],
      "metadata": {
        "id": "HBGoGa3yQ1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQnFbnHdSWLN",
        "outputId": "6e1b6417-b654-4ef9-f195-49a449ac496d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepdriver==0.0.54"
      ],
      "metadata": {
        "id": "ep81qe8Cs3oZ",
        "outputId": "5e09b045-da9c-47c8-9052-b4f53123d09f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepdriver==0.0.54\n",
            "  Downloading deepdriver-0.0.54-py3-none-any.whl (30 kB)\n",
            "Collecting assertpy\n",
            "  Downloading assertpy-1.1.tar.gz (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (1.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (0.38.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (1.51.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (7.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (5.4.8)\n",
            "Collecting grpcio-tools\n",
            "  Downloading grpcio_tools-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (5.5.0)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.0.54) (2.23.0)\n",
            "Collecting protobuf<5.0dev,>=4.21.6\n",
            "  Downloading protobuf-4.21.11-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[K     |████████████████████████████████| 409 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from grpcio-tools->deepdriver==0.0.54) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.0.54) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.0.54) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->deepdriver==0.0.54) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->deepdriver==0.0.54) (8.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.0.54) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.0.54) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.0.54) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.0.54) (1.24.3)\n",
            "Building wheels for collected packages: assertpy\n",
            "  Building wheel for assertpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for assertpy: filename=assertpy-1.1-py3-none-any.whl size=42917 sha256=5a94d76c591327937a69be79ff6655552197c76b1662e55004ff876e56cdbaa8\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/86/c9/1310be6ddfb540daa0bf1ac204526837aa0a8b0e79f32855ff\n",
            "Successfully built assertpy\n",
            "Installing collected packages: protobuf, pynvml, grpcio-tools, assertpy, deepdriver\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.11 which is incompatible.\n",
            "tensorflow-metadata 1.11.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.11 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.11 which is incompatible.\u001b[0m\n",
            "Successfully installed assertpy-1.1 deepdriver-0.0.54 grpcio-tools-1.51.1 protobuf-4.21.11 pynvml-11.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install protobuf==3.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "zPlF7uZyT6LN",
        "outputId": "1e250cb5-423a-483f-f927-6c6a24851412"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 6.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.21.11\n",
            "    Uninstalling protobuf-4.21.11:\n",
            "      Successfully uninstalled protobuf-4.21.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpcio-tools 1.51.1 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.57.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.9.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.3.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.16.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\n",
            "Successfully installed protobuf-3.20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. ➕ import deepdriver & deeplearnig framework\n",
        "\n"
      ],
      "metadata": {
        "id": "XHyfUGRdRGJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t2G8rFYSMxe8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepdriver"
      ],
      "metadata": {
        "id": "nMIZhc0aye6k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. prepare dataset"
      ],
      "metadata": {
        "id": "bW1624UeFQS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "-O /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p64rEhWJI01",
        "outputId": "94a01604-9d9e-4e7f-d505-5547c694a9b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-14 09:32:47--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 173.194.202.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   188MB/s    in 0.3s    \n",
            "\n",
            "2022-12-14 09:32:47 (188 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "train_dir = '/content/cat_dog/'\n",
        "if os.path.isdir(train_dir):\n",
        "  shutil.rmtree(train_dir)\n",
        "else:\n",
        "  os.mkdir(train_dir)\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "data_dir ='/tmp/cats_and_dogs_filtered/train'\n",
        "data_doc_dir='/tmp/cats_and_dogs_filtered/train/dogs'\n",
        "data_cat_dir='/tmp/cats_and_dogs_filtered/train/cats'\n",
        "valid_dir ='/tmp/cats_and_dogs_filtered/validation'\n",
        "data_dog_valid_dir='/tmp/cats_and_dogs_filtered/validation/dogs'\n",
        "data_cat_valid_dir='/tmp/cats_and_dogs_filtered/validation/cats'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "zhLshy2aFOtU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train용 폴더 생성\n",
        "train_set_dir = os.path.join(train_dir, 'train_set')\n",
        "os.mkdir(train_set_dir)\n",
        "train_dog_dir = os.path.join(train_set_dir, 'dog')\n",
        "os.mkdir(train_dog_dir)\n",
        "train_cat_dir = os.path.join(train_set_dir, 'cat')\n",
        "os.mkdir(train_cat_dir)\n",
        "# valid용 폴더 생성\n",
        "valid_set_dir = os.path.join(train_dir, 'valid_set')\n",
        "os.mkdir(valid_set_dir)\n",
        "valid_dog_dir = os.path.join(valid_set_dir, 'dog')\n",
        "os.mkdir(valid_dog_dir)\n",
        "valid_cat_dir = os.path.join(valid_set_dir, 'cat')\n",
        "os.mkdir(valid_cat_dir)\n",
        "# test용 폴더 생성\n",
        "test_set_dir = os.path.join(train_dir, 'test_set')\n",
        "os.mkdir(test_set_dir)\n",
        "test_dog_dir = os.path.join(test_set_dir, 'dog')\n",
        "os.mkdir(test_dog_dir)\n",
        "test_cat_dir = os.path.join(test_set_dir, 'cat')\n",
        "os.mkdir(test_cat_dir)"
      ],
      "metadata": {
        "id": "v2zaZn9DJXgL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image file name list 생성\n",
        "import shutil\n",
        "total_data_count =1000\n",
        "train_data_count =int(total_data_count*0.8)\n",
        "test_data_count = int(total_data_count*0.1)\n",
        "valid_data_count =total_data_count - train_data_count -test_data_count\n",
        "dog_files = [f'dog.{i}.jpg' for i in range(total_data_count)]\n",
        "cat_files = [f'cat.{i}.jpg' for i in range(total_data_count)]\n",
        "\n",
        " \n",
        "# 각 폴더로 image 이동\n",
        "for file in dog_files[:train_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(train_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in dog_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(valid_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in dog_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(test_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[:train_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(train_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in cat_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(valid_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(test_cat_dir, file)\n",
        "    shutil.move(src, dst)"
      ],
      "metadata": {
        "id": "jtZQt0odOq_U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_fnames = os.listdir( train_cat_dir )\n",
        "train_dog_fnames = os.listdir( train_dog_dir )"
      ],
      "metadata": {
        "id": "zwuVLt8dRf4q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total training cat images :', len(os.listdir(train_cat_dir)))\n",
        "print('Total training dog images :', len(os.listdir(train_dog_dir)))\n",
        "\n",
        "print('Total validation cat images :', len(os.listdir(valid_cat_dir)))\n",
        "print('Total validation dog images :', len(os.listdir(valid_dog_dir)))\n",
        "\n",
        "\n",
        "print('Total test cat images :', len(os.listdir(test_cat_dir)))\n",
        "print('Total test dog images :', len(os.listdir(test_dog_dir)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWtzjLavRlLj",
        "outputId": "843e6d18-f263-4e71-e7f7-23964dfc92b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training cat images : 800\n",
            "Total training dog images : 800\n",
            "Total validation cat images : 100\n",
            "Total validation dog images : 100\n",
            "Total test cat images : 100\n",
            "Total test dog images : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. ⚙ deepdriver server setting"
      ],
      "metadata": {
        "id": "FG0DGtb2VhXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.setting(http_host=\"quick-experience.bokchi.com:9011\" ,grpc_host=\"quick-experience.bokchi.com:19051\")"
      ],
      "metadata": {
        "id": "XDfF___IVcC2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. 🔌 log in to deepdriver"
      ],
      "metadata": {
        "id": "36Y_KWmfRYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.login(key=\"MGY2ZWY5NjY0NjE3YjVkODBhNGRkYjBkZDAzYzQ5MGMxMzVhZWRhNzkyYTdiNDI4ZGZmYjZmZDhmYzdkY2I3ZQ==\")"
      ],
      "metadata": {
        "id": "hF3GenoNsaBQ",
        "outputId": "48b9f8fc-06d5-4935-e438-6ac47ce91a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. 🥼 create experiment and run"
      ],
      "metadata": {
        "id": "gWafO8QnRpc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment init & config hyperparam\n",
        "deepdriver.init(exp_name= \"cat_dog_cnn\", \n",
        "                config={ 'architecture':\"CNN\", 'epoch': 15, 'batch_size': 128, 'hidden_layer':512, 'learning_rate': 0.001})"
      ],
      "metadata": {
        "id": "z93NrRBUsvDm",
        "outputId": "5d361739-e8cb-4665-a19e-9060d1a3a0ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-14 09:36:18,419 INFO [deepdriver] [experiment.py:21] - DeepDriver initialized\n",
            "Team Name=molamola.babo\n",
            "Exp Name=cat_dog_cnn\n",
            "Run Name=run-12\n",
            "Run URL=http://quick-experience.bokchi.com:9111/experi/molamola.babo/cat_dog_cnn/run-12/run/chart\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:deepdriver:DeepDriver initialized\n",
            "Team Name=molamola.babo\n",
            "Exp Name=cat_dog_cnn\n",
            "Run Name=run-12\n",
            "Run URL=http://quick-experience.bokchi.com:9111/experi/molamola.babo/cat_dog_cnn/run-12/run/chart\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<deepdriver.sdk.data_types.run.Run at 0x7f1dd62e6dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. 📚 train your code and send log"
      ],
      "metadata": {
        "id": "CNDciDDURuZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model"
      ],
      "metadata": {
        "id": "wbFA68eYps1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(deepdriver.config.hidden_layer, activation='relu'),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qga-dT4iRyR6",
        "outputId": "10cfa899-c2d1-4b76-e0e5-319c8a4645c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 148, 148, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 74, 74, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 72, 72, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 36, 36, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 17, 17, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18496)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               9470464   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,494,561\n",
            "Trainable params: 9,494,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=deepdriver.config.learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "dHinKbNnRyUv",
        "outputId": "3921500b-f3a0-4031-d49b-3df973b51563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data load"
      ],
      "metadata": {
        "id": "CHW9p5NypwdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "valid_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_set_dir,\n",
        "                                                  batch_size=deepdriver.config.batch_size,\n",
        "                                                  class_mode='binary',\n",
        "                                                  target_size=(150, 150))\n",
        "validation_generator =  valid_datagen.flow_from_directory(valid_set_dir,\n",
        "                                                       batch_size=deepdriver.config.batch_size,\n",
        "                                                       class_mode  = 'binary',\n",
        "                                                       target_size = (150, 150))\n",
        "\n",
        "test_generator =  test_datagen.flow_from_directory(test_set_dir,\n",
        "                                                       batch_size=deepdriver.config.batch_size,\n",
        "                                                       class_mode  = 'binary',\n",
        "                                                       target_size = (150, 150))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3x9kB6MVClP",
        "outputId": "29bfeaf7-51da-451e-f27b-f51113c5f44a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1600 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add deepdriver.log() in train function"
      ],
      "metadata": {
        "id": "3ecGvoxzp-Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # def on_train_end(self, logs=None):\n",
        "    #     keys = list(logs.keys())\n",
        "    #     deepdriver.finish()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        deepdriver.log({\"acc\": logs[\"accuracy\"], \"loss\": logs[\"loss\"], \"val_acc\": logs[\"val_accuracy\"], \"val_loss\": logs[\"val_loss\"]})\n",
        "        #deepdriver.log(logs)\n"
      ],
      "metadata": {
        "id": "O9oUAVd0X8qS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train model"
      ],
      "metadata": {
        "id": "6JmoZzikqNRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = train_generator.n//deepdriver.config.batch_size"
      ],
      "metadata": {
        "id": "2zXQzAUUZP1C"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_steps = validation_generator.n//deepdriver.config.batch_size"
      ],
      "metadata": {
        "id": "rI4UVyABZQcy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    epochs=deepdriver.config.epoch,\n",
        "                    validation_steps=validation_steps ,\n",
        "                    callbacks=[CustomCallback()],\n",
        "                    verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF8hsCeJXcIt",
        "outputId": "2d0f72e6-540d-4f13-fde0-c98eb0ca6edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "12/12 - 14s - loss: 1.9575 - accuracy: 0.4993 - val_loss: 0.6846 - val_accuracy: 0.6094 - 14s/epoch - 1s/step\n",
            "Epoch 2/15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"cat_dog_cnn_model\")"
      ],
      "metadata": {
        "id": "kPeoZ7K5YXSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. ⬆ upload artifact(Model)"
      ],
      "metadata": {
        "id": "b4SVJ4HcKugj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arti = deepdriver.Artifacts(name=\"cat_dog_cnn_model\",type=\"model\")"
      ],
      "metadata": {
        "id": "TxgovR2JKt4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti.add(\"cat_dog_cnn_model\")"
      ],
      "metadata": {
        "id": "hhsY3_lVLR6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[ent.path for ent  in arti.entry_list]"
      ],
      "metadata": {
        "id": "rJ0sHNYM0oOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti.upload()"
      ],
      "metadata": {
        "id": "Ojc77RQKbi8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.finish()"
      ],
      "metadata": {
        "id": "36ARN62es0rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. ⬇ create another run & download artifact(Model)"
      ],
      "metadata": {
        "id": "Ls72Ap0WLywJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment init & config hyperparam\n",
        "deepdriver.init(exp_name= \"cat_dog_cnn\", \n",
        "                config={ 'architecture':\"CNN\", 'epoch': 5, 'batch_size': 128, 'hidden_layer':512, 'learning_rate': 0.001})"
      ],
      "metadata": {
        "id": "WIs3oC7asCP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti2 = deepdriver.get_artifact(name=\"cat_dog_cnn_model\",type=\"model\")"
      ],
      "metadata": {
        "id": "b3e58_uUL5gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti2.download()"
      ],
      "metadata": {
        "id": "7gSoGrP9MNls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. model re-train & upload model with another version"
      ],
      "metadata": {
        "id": "GqP6VGdVyFvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arti2.get"
      ],
      "metadata": {
        "id": "qIYaVfhw90cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reload_model = tf.keras.models.load_model(arti2.get_download_dir())"
      ],
      "metadata": {
        "id": "49Cz_wrXiyIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = reload_model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    epochs=deepdriver.config.epoch,\n",
        "                    validation_steps=validation_steps ,\n",
        "                    callbacks=[CustomCallback()],\n",
        "                    verbose=2)"
      ],
      "metadata": {
        "id": "VS0s5I91l225"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reload_model.save(\"cat_dog_cnn_model2\")"
      ],
      "metadata": {
        "id": "ff_2bMA2oI0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti = deepdriver.get_artifact(name=\"cat_dog_cnn_model\",type=\"model\")"
      ],
      "metadata": {
        "id": "bFtHE3IroQDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti.add(\"cat_dog_cnn_model2\")"
      ],
      "metadata": {
        "id": "hBTWUVB-oXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti.upload()"
      ],
      "metadata": {
        "id": "AaFL4-dhodJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.finish()"
      ],
      "metadata": {
        "id": "CGRnS7qPs55S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fhh84MKyRN-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}