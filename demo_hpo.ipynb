{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molabokchi/bokchi_open_lab/blob/main/demo_hpo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ![다운로드.png](data:image/png;base64,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) deepdriver quickstart!"
      ],
      "metadata": {
        "id": "ccBiuOu0rtin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "welecome to deepdriver! 😀"
      ],
      "metadata": {
        "id": "ZGV9WnWBrvb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can refer to the guide document.\n",
        "https://bokchi.gitbook.io/deepdriver-ce/"
      ],
      "metadata": {
        "id": "RuepMN3Yyuu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. 💻 install deepdriver & requirement package for train"
      ],
      "metadata": {
        "id": "HBGoGa3yQ1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQnFbnHdSWLN",
        "outputId": "9ca705b7-7e68-4e66-da6d-9bb1a2c86a68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepdriver==0.4.6"
      ],
      "metadata": {
        "id": "ep81qe8Cs3oZ",
        "outputId": "c079c705-5f48-4710-c8be-b3354e4819fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepdriver==0.4.6\n",
            "  Downloading deepdriver-0.4.6-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (1.3.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (5.4.8)\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.5-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psycopg2 in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (2.9.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (2.25.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (0.38.4)\n",
            "Collecting assertpy\n",
            "  Downloading assertpy-1.1.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (1.21.6)\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 KB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools\n",
            "  Downloading grpcio_tools-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.4.6) (1.51.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from grpcio-tools->deepdriver==0.4.6) (57.4.0)\n",
            "Collecting protobuf<5.0dev,>=4.21.6\n",
            "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.8/409.8 KB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cliff\n",
            "  Downloading cliff-4.1.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.1-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.4/210.4 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.4.6) (4.64.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.4.6) (1.4.46)\n",
            "Collecting importlib-metadata<5.0.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.4.6) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.4.6) (21.3)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.4.6) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.4.6) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->deepdriver==0.4.6) (8.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly->deepdriver==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.4.6) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.4.6) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.4.6) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.4.6) (1.24.3)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna->deepdriver==0.4.6) (5.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata<5.0.0->optuna->deepdriver==0.4.6) (3.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna->deepdriver==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna->deepdriver==0.4.6) (2.0.1)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-4.1.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.1/147.1 KB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.8/dist-packages (from cliff->optuna->deepdriver==0.4.6) (3.5.0)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepdriver==0.4.6) (22.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepdriver==0.4.6) (0.2.5)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 KB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna->deepdriver==0.4.6) (2.0.1)\n",
            "Building wheels for collected packages: assertpy, pyperclip\n",
            "  Building wheel for assertpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for assertpy: filename=assertpy-1.1-py3-none-any.whl size=42917 sha256=f49db5001d3851b6c887e593da68f9bff61b8c2aff76da391358dc0b3e3c42c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/86/c9/1310be6ddfb540daa0bf1ac204526837aa0a8b0e79f32855ff\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=55aa986b1c8f4f60623be5c9025326513e90cab7ea05031018356bf2f75b3547\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/1a/65/84ff8c386bec21fca6d220ea1f5498a0367883a78dd5ba6122\n",
            "Successfully built assertpy pyperclip\n",
            "Installing collected packages: pyperclip, assertpy, pynvml, protobuf, pbr, Mako, importlib-metadata, colorlog, cmd2, cmaes, autopage, stevedore, grpcio-tools, alembic, cliff, optuna, deepdriver\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.0.0\n",
            "    Uninstalling importlib-metadata-6.0.0:\n",
            "      Successfully uninstalled importlib-metadata-6.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.2.4 alembic-1.9.1 assertpy-1.1 autopage-0.5.1 cliff-4.1.0 cmaes-0.9.1 cmd2-2.4.2 colorlog-6.7.0 deepdriver-0.4.6 grpcio-tools-1.51.1 importlib-metadata-4.13.0 optuna-3.0.5 pbr-5.11.0 protobuf-4.21.12 pynvml-11.4.1 pyperclip-1.8.2 stevedore-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install protobuf==3.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPlF7uZyT6LN",
        "outputId": "b7791a5f-c9a3-42d2-d50f-b0d8773a3a39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.0\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.21.12\n",
            "    Uninstalling protobuf-4.21.12:\n",
            "      Successfully uninstalled protobuf-4.21.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpcio-tools 1.51.1 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.57.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.17.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. ➕ import deepdriver & deeplearnig framework\n",
        "\n"
      ],
      "metadata": {
        "id": "XHyfUGRdRGJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t2G8rFYSMxe8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepdriver"
      ],
      "metadata": {
        "id": "nMIZhc0aye6k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. prepare dataset"
      ],
      "metadata": {
        "id": "bW1624UeFQS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "-O /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p64rEhWJI01",
        "outputId": "c05eb0de-516e-4b90-fcf6-1b96a00c9dd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-11 07:48:35--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.68.128, 74.125.24.128, 142.250.4.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.68.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  20.6MB/s    in 3.8s    \n",
            "\n",
            "2023-01-11 07:48:40 (17.1 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "train_dir = '/content/cat_dog/'\n",
        "if os.path.isdir(train_dir):\n",
        "  shutil.rmtree(train_dir)\n",
        "  os.mkdir(train_dir)\n",
        "else:\n",
        "  os.mkdir(train_dir)\n",
        "\n",
        "model_dir = '/content/model'\n",
        "\n",
        "if os.path.isdir(model_dir):\n",
        "  shutil.rmtree(model_dir)\n",
        "  os.mkdir(model_dir)\n",
        "else:\n",
        "  os.mkdir(model_dir)\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "data_dir ='/tmp/cats_and_dogs_filtered/train'\n",
        "data_doc_dir='/tmp/cats_and_dogs_filtered/train/dogs'\n",
        "data_cat_dir='/tmp/cats_and_dogs_filtered/train/cats'\n",
        "valid_dir ='/tmp/cats_and_dogs_filtered/validation'\n",
        "data_dog_valid_dir='/tmp/cats_and_dogs_filtered/validation/dogs'\n",
        "data_cat_valid_dir='/tmp/cats_and_dogs_filtered/validation/cats'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "zhLshy2aFOtU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train용 폴더 생성\n",
        "train_set_dir = os.path.join(train_dir, 'train_set')\n",
        "os.mkdir(train_set_dir)\n",
        "train_dog_dir = os.path.join(train_set_dir, 'dog')\n",
        "os.mkdir(train_dog_dir)\n",
        "train_cat_dir = os.path.join(train_set_dir, 'cat')\n",
        "os.mkdir(train_cat_dir)\n",
        "# valid용 폴더 생성\n",
        "valid_set_dir = os.path.join(train_dir, 'valid_set')\n",
        "os.mkdir(valid_set_dir)\n",
        "valid_dog_dir = os.path.join(valid_set_dir, 'dog')\n",
        "os.mkdir(valid_dog_dir)\n",
        "valid_cat_dir = os.path.join(valid_set_dir, 'cat')\n",
        "os.mkdir(valid_cat_dir)\n",
        "# test용 폴더 생성\n",
        "test_set_dir = os.path.join(train_dir, 'test_set')\n",
        "os.mkdir(test_set_dir)\n",
        "test_dog_dir = os.path.join(test_set_dir, 'dog')\n",
        "os.mkdir(test_dog_dir)\n",
        "test_cat_dir = os.path.join(test_set_dir, 'cat')\n",
        "os.mkdir(test_cat_dir)"
      ],
      "metadata": {
        "id": "v2zaZn9DJXgL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image file name list 생성\n",
        "import shutil\n",
        "total_data_count =1000\n",
        "train_data_count =int(total_data_count*0.8)\n",
        "test_data_count = int(total_data_count*0.1)\n",
        "valid_data_count =total_data_count - train_data_count -test_data_count\n",
        "dog_files = [f'dog.{i}.jpg' for i in range(total_data_count)]\n",
        "cat_files = [f'cat.{i}.jpg' for i in range(total_data_count)]\n",
        "\n",
        " \n",
        "# 각 폴더로 image 이동\n",
        "for file in dog_files[:train_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(train_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in dog_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(valid_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in dog_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(test_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[:train_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(train_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in cat_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(valid_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(test_cat_dir, file)\n",
        "    shutil.move(src, dst)"
      ],
      "metadata": {
        "id": "jtZQt0odOq_U"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_cat_fnames = os.listdir( train_cat_dir )\n",
        "train_dog_fnames = os.listdir( train_dog_dir )"
      ],
      "metadata": {
        "id": "zwuVLt8dRf4q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total training cat images :', len(os.listdir(train_cat_dir)))\n",
        "print('Total training dog images :', len(os.listdir(train_dog_dir)))\n",
        "\n",
        "print('Total validation cat images :', len(os.listdir(valid_cat_dir)))\n",
        "print('Total validation dog images :', len(os.listdir(valid_dog_dir)))\n",
        "\n",
        "\n",
        "print('Total test cat images :', len(os.listdir(test_cat_dir)))\n",
        "print('Total test dog images :', len(os.listdir(test_dog_dir)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWtzjLavRlLj",
        "outputId": "7f0fabff-f0cc-422b-8c0e-9d3847301e9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total training cat images : 800\n",
            "Total training dog images : 800\n",
            "Total validation cat images : 100\n",
            "Total validation dog images : 100\n",
            "Total test cat images : 100\n",
            "Total test dog images : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. ⚙ deepdriver server setting"
      ],
      "metadata": {
        "id": "FG0DGtb2VhXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.setting(http_host=\"54.180.86.146:9011\" ,grpc_host=\"54.180.86.146:19051\")"
      ],
      "metadata": {
        "id": "XDfF___IVcC2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. 🔌 log in to deepdriver"
      ],
      "metadata": {
        "id": "36Y_KWmfRYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.login(key=\"ZmIyNWQxNGJkMzUxYTVjODQ2NjM5NTgzOTM0YTM2OGE2ZmJiY2M2MWMwOWQ0OWFkNjU2YzNkM2UxMjA0YTVkZg==\")"
      ],
      "metadata": {
        "id": "hF3GenoNsaBQ",
        "outputId": "0fb8103f-a740-4076-acdf-24e260855ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. 🥼 create hpo"
      ],
      "metadata": {
        "id": "gWafO8QnRpc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name=\"cat_dog_cnn_hpo3\""
      ],
      "metadata": {
        "id": "6dW_6K0Pq2xm"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpo_configuration = {\n",
        "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
        "    \"method\": \"grid\",\n",
        "    \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [16,32,64,128]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"range\": [0.01, 0.001]\n",
        "        },\n",
        "        \"hidden_layer\":{\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"epoch\" :{\n",
        "            \"values\": [5,10]\n",
        "        }\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Mm23Iq3NXa7x"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment init & config hyperparam\n",
        "deepdriver.create_hpo(exp_name= experiment_name,hpo_config= hpo_configuration)"
      ],
      "metadata": {
        "id": "z93NrRBUsvDm",
        "outputId": "125af46e-23d9-4503-8d27-f7b1acde17c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-5d52fa0261dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# experiment init & config hyperparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeepdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_hpo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhpo_config\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhpo_configuration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"please log in first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Login required.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\u001b[0m in \u001b[0;36mcreate_hpo\u001b[0;34m(exp_name, team_name, remote, hpo_config)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mparameters_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhpo_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mkey_value_parameters_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             key_value_dict = {\n\u001b[1;32m     56\u001b[0m                 \u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. 📚 train your code and send log"
      ],
      "metadata": {
        "id": "CNDciDDURuZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model"
      ],
      "metadata": {
        "id": "wbFA68eYps1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(hidden_layer, learning_rate):\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(hidden_layer, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "  model.compile(optimizer=RMSprop(lr=learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics = ['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "335yovudanHT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(batch_size):\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "  train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "  test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "  valid_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "  train_generator = train_datagen.flow_from_directory(train_set_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "  validation_generator =  valid_datagen.flow_from_directory(valid_set_dir,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        class_mode  = 'binary',\n",
        "                                                        target_size = (150, 150))\n",
        "\n",
        "  test_generator =  test_datagen.flow_from_directory(test_set_dir,\n",
        "                                                        batch_size=batch_size,\n",
        "                                                        class_mode  = 'binary',\n",
        "                                                        target_size = (150, 150))\n",
        "  return train_generator, validation_generator, test_generator"
      ],
      "metadata": {
        "id": "beFx8bZya61B"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  import tensorflow as tf \n",
        "  model = define_model(deepdriver.config.hidden_layer, deepdriver.config.learning_rate)\n",
        "  \n",
        "  train_generator, validation_generator, test_generator = prepare_data(deepdriver.config.batch_size)\n",
        "\n",
        "  class CustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    # def on_train_end(self, logs=None):\n",
        "    #     keys = list(logs.keys())\n",
        "    #     deepdriver.finish()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        keys = list(logs.keys())\n",
        "        deepdriver.log({\"acc\": logs[\"accuracy\"], \"loss\": logs[\"loss\"], \"val_acc\": logs[\"val_accuracy\"], \"val_loss\": logs[\"val_loss\"]})\n",
        "        #deepdriver.log(logs)\n",
        "\n",
        "  steps_per_epoch = train_generator.n//deepdriver.config.batch_size\n",
        "  validation_steps = validation_generator.n//deepdriver.config.batch_size\n",
        "\n",
        "  history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    steps_per_epoch=steps_per_epoch,\n",
        "                    epochs=deepdriver.config.epoch,\n",
        "                    validation_steps=validation_steps ,\n",
        "                    callbacks=[CustomCallback()],\n",
        "                    verbose=2)\n",
        "  result = model.evaluate(test_generator)\n",
        "\n",
        "  return result[\"accuracy\"]\n"
      ],
      "metadata": {
        "id": "I817oYvyaNWv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " deepdriver.run_hpo(exp_name=experiment_name, func=train, count=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4GuWFXW1rHX",
        "outputId": "a5cc60a5-bc67-43bf-c556-2d94853bdc07"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepDriver initialized\n",
            "Team Name=molamola.python\n",
            "Exp Name=cat_dog_cnn_hpo2\n",
            "Run Name=run-1\n",
            "Run URL=http://54.180.86.146:9111/experi/molamola.python/cat_dog_cnn_hpo2/run-1/run/chart\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2023-01-11 08:05:03,036]\u001b[0m Trial 0 failed because of the following error: AttributeError(\"'Config' object has no attribute 'hidden_layer'\")\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 139, in <lambda>\n",
            "    return lambda trial: objective(trial, exp_name, team_name, func, config)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 144, in objective\n",
            "    y = func()\n",
            "  File \"<ipython-input-40-634c0d8e468c>\", line 3, in train\n",
            "    model = define_model()\n",
            "  File \"<ipython-input-38-16c41a93b821>\", line 11, in define_model\n",
            "    tf.keras.layers.Dense(deepdriver.config.hidden_layer, activation='relu'),\n",
            "AttributeError: 'Config' object has no attribute 'hidden_layer'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Config' object has no attribute 'hidden_layer'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 160, in run_optimize\n",
            "    study = study.optimize(get_wrapped_func(exp_name, team_name_, func, config), n_trials=count)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/study.py\", line 419, in optimize\n",
            "    _optimize(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 66, in _optimize\n",
            "    _optimize_sequential(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
            "    frozen_trial = _run_trial(study, func, catch)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 234, in _run_trial\n",
            "    raise func_err\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 139, in <lambda>\n",
            "    return lambda trial: objective(trial, exp_name, team_name, func, config)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 144, in objective\n",
            "    y = func()\n",
            "  File \"<ipython-input-40-634c0d8e468c>\", line 3, in train\n",
            "    model = define_model()\n",
            "  File \"<ipython-input-38-16c41a93b821>\", line 11, in define_model\n",
            "    tf.keras.layers.Dense(deepdriver.config.hidden_layer, activation='relu'),\n",
            "AttributeError: 'Config' object has no attribute 'hidden_layer'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dn_UkcTT2kvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}