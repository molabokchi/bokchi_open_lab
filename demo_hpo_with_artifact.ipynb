{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molabokchi/bokchi_open_lab/blob/main/demo_hpo_with_artifact.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ![Îã§Ïö¥Î°úÎìú.png](data:image/png;base64,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) deepdriver hpo quickstart!"
      ],
      "metadata": {
        "id": "ccBiuOu0rtin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "welecome to deepdriver! üòÄ"
      ],
      "metadata": {
        "id": "ZGV9WnWBrvb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can refer to the guide document.\n",
        "https://bokchi.gitbook.io/deepdriver-ce/"
      ],
      "metadata": {
        "id": "RuepMN3Yyuu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. üíª install deepdriver & requirement package for train"
      ],
      "metadata": {
        "id": "HBGoGa3yQ1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQnFbnHdSWLN",
        "outputId": "5a0868fd-c003-4f2c-9254-eb1596530144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Collecting protobuf<3.20,>=3.9.2\n",
            "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.29.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.0\n",
            "    Uninstalling protobuf-3.20.0:\n",
            "      Successfully uninstalled protobuf-3.20.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-tools 1.51.1 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.19.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install deepdriver==0.7.5"
      ],
      "metadata": {
        "id": "ep81qe8Cs3oZ",
        "outputId": "a5de67a5-d774-4d42-97ff-bac284d2414f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepdriver==0.7.3 in /usr/local/lib/python3.8/dist-packages (0.7.3)\n",
            "Requirement already satisfied: grpcio-tools in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (1.51.1)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (1.51.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (7.1.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (5.5.0)\n",
            "Requirement already satisfied: assertpy in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (2.25.1)\n",
            "Requirement already satisfied: psycopg2 in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (2.9.5)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (3.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (5.4.8)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (11.4.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (0.38.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from deepdriver==0.7.3) (1.3.5)\n",
            "Collecting protobuf<5.0dev,>=4.21.6\n",
            "  Using cached protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from grpcio-tools->deepdriver==0.7.3) (57.4.0)\n",
            "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (21.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (6.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (6.7.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (1.4.46)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (1.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna->deepdriver==0.7.3) (4.64.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.7.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->deepdriver==0.7.3) (2022.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly->deepdriver==0.7.3) (8.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly->deepdriver==0.7.3) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.7.3) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.7.3) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.7.3) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->deepdriver==0.7.3) (2.10)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna->deepdriver==0.7.3) (1.2.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna->deepdriver==0.7.3) (6.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna->deepdriver==0.7.3) (5.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna->deepdriver==0.7.3) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna->deepdriver==0.7.3) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna->deepdriver==0.7.3) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna->deepdriver==0.7.3) (2.0.1)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.12 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.21.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install protobuf==3.20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPlF7uZyT6LN",
        "outputId": "4ecc9256-6553-40d4-f975-165974c988b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.0\n",
            "  Using cached protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.21.12\n",
            "    Uninstalling protobuf-4.21.12:\n",
            "      Successfully uninstalled protobuf-4.21.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.9.2 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.0 which is incompatible.\n",
            "grpcio-tools 1.51.1 requires protobuf<5.0dev,>=4.21.6, but you have protobuf 3.20.0 which is incompatible.\n",
            "googleapis-common-protos 1.58.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-translate 3.8.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-language 2.6.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-firestore 2.7.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-datastore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery 3.4.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.17.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfcPYMuKmwab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. ‚ûï import deepdriver & deeplearnig framework\n",
        "\n"
      ],
      "metadata": {
        "id": "XHyfUGRdRGJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "t2G8rFYSMxe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deepdriver"
      ],
      "metadata": {
        "id": "nMIZhc0aye6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. ‚öô deepdriver server setting"
      ],
      "metadata": {
        "id": "FG0DGtb2VhXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.setting(http_host=\"54.180.86.146:9011\" ,grpc_host=\"54.180.86.146:19051\")"
      ],
      "metadata": {
        "id": "XDfF___IVcC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. üîå log in to deepdriver"
      ],
      "metadata": {
        "id": "36Y_KWmfRYwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.login(key=\"ZmIyNWQxNGJkMzUxYTVjODQ2NjM5NTgzOTM0YTM2OGE2ZmJiY2M2MWMwOWQ0OWFkNjU2YzNkM2UxMjA0YTVkZg==\")"
      ],
      "metadata": {
        "id": "hF3GenoNsaBQ",
        "outputId": "a74938f7-0e58-41d5-b1ea-919d1013d0ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. ‚å® upload data & code ( if you upload aleady, skip)"
      ],
      "metadata": {
        "id": "R5Mb2C2docNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/molabokchi/bokchi_open_lab/main/main.py -O main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C84yhBQ2UMuX",
        "outputId": "35f38cec-cb88-41d7-c8eb-611ce7bce3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-27 00:41:54--  https://raw.githubusercontent.com/molabokchi/bokchi_open_lab/main/main.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3671 (3.6K) [text/plain]\n",
            "Saving to: ‚Äòmain.py‚Äô\n",
            "\n",
            "\rmain.py               0%[                    ]       0  --.-KB/s               \rmain.py             100%[===================>]   3.58K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-27 00:41:54 (46.9 MB/s) - ‚Äòmain.py‚Äô saved [3671/3671]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.init(exp_name=\"upload_data_code\")"
      ],
      "metadata": {
        "id": "Hl44wF5Wn3Ii",
        "outputId": "4195a459-f922-4f0b-f5fa-0181ffb6a0e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepDriver initialized\n",
            "Team Name=molamola.python\n",
            "Exp Name=upload_data_code\n",
            "Run Name=run-19\n",
            "Run URL=http://54.180.86.146:9111/experi/molamola.python/upload_data_code/run-19/run/chart\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<deepdriver.sdk.data_types.run.Run at 0x7f7014dc8a00>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.upload_code(name=\"cnn_image_3\",type=\"CODE\",path=\"./\")"
      ],
      "metadata": {
        "id": "qTfhH_Gx5Mvr",
        "outputId": "5266bb6f-0948-4a49-edf8-483939537937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artifact is created!\n",
            "Uploading: [./main.py] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100.0%] [1/1]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "-O /tmp/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "id": "YMzMv1F6qBs8",
        "outputId": "95c7c6cf-c43e-4061-f260-7c34cec26aed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-27 00:36:14--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.194.128, 173.194.196.128, 173.194.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.194.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‚Äò/tmp/cats_and_dogs_filtered.zip‚Äô\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   184MB/s    in 0.4s    \n",
            "\n",
            "2023-01-27 00:36:14 (184 MB/s) - ‚Äò/tmp/cats_and_dogs_filtered.zip‚Äô saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "train_dir = '/content/cat_dog/'\n",
        "if os.path.isdir(train_dir):\n",
        "  shutil.rmtree(train_dir)\n",
        "  os.mkdir(train_dir)\n",
        "else:\n",
        "  os.mkdir(train_dir)\n",
        "\n",
        "model_dir = '/content/model'\n",
        "\n",
        "if os.path.isdir(model_dir):\n",
        "  shutil.rmtree(model_dir)\n",
        "  os.mkdir(model_dir)\n",
        "else:\n",
        "  os.mkdir(model_dir)\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "data_dir ='/tmp/cats_and_dogs_filtered/train'\n",
        "data_doc_dir='/tmp/cats_and_dogs_filtered/train/dogs'\n",
        "data_cat_dir='/tmp/cats_and_dogs_filtered/train/cats'\n",
        "valid_dir ='/tmp/cats_and_dogs_filtered/validation'\n",
        "data_dog_valid_dir='/tmp/cats_and_dogs_filtered/validation/dogs'\n",
        "data_cat_valid_dir='/tmp/cats_and_dogs_filtered/validation/cats'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "H3GCfqYLqnMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainÏö© Ìè¥Îçî ÏÉùÏÑ±\n",
        "train_set_dir = os.path.join(train_dir, 'train_set')\n",
        "os.mkdir(train_set_dir)\n",
        "train_dog_dir = os.path.join(train_set_dir, 'dog')\n",
        "os.mkdir(train_dog_dir)\n",
        "train_cat_dir = os.path.join(train_set_dir, 'cat')\n",
        "os.mkdir(train_cat_dir)\n",
        "# validÏö© Ìè¥Îçî ÏÉùÏÑ±\n",
        "valid_set_dir = os.path.join(train_dir, 'valid_set')\n",
        "os.mkdir(valid_set_dir)\n",
        "valid_dog_dir = os.path.join(valid_set_dir, 'dog')\n",
        "os.mkdir(valid_dog_dir)\n",
        "valid_cat_dir = os.path.join(valid_set_dir, 'cat')\n",
        "os.mkdir(valid_cat_dir)\n",
        "# testÏö© Ìè¥Îçî ÏÉùÏÑ±\n",
        "test_set_dir = os.path.join(train_dir, 'test_set')\n",
        "os.mkdir(test_set_dir)\n",
        "test_dog_dir = os.path.join(test_set_dir, 'dog')\n",
        "os.mkdir(test_dog_dir)\n",
        "test_cat_dir = os.path.join(test_set_dir, 'cat')\n",
        "os.mkdir(test_cat_dir)"
      ],
      "metadata": {
        "id": "96f2QcacqptF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image file name list ÏÉùÏÑ±\n",
        "import shutil\n",
        "total_data_count =1000\n",
        "train_data_count =int(total_data_count*0.8)\n",
        "test_data_count = int(total_data_count*0.1)\n",
        "valid_data_count =total_data_count - train_data_count -test_data_count\n",
        "dog_files = [f'dog.{i}.jpg' for i in range(total_data_count)]\n",
        "cat_files = [f'cat.{i}.jpg' for i in range(total_data_count)]\n",
        "\n",
        " \n",
        "# Í∞Å Ìè¥ÎçîÎ°ú image Ïù¥Îèô\n",
        "for file in dog_files[:train_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(train_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in dog_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(valid_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in dog_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_doc_dir, file)\n",
        "    dst = os.path.join(test_dog_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[:train_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(train_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        "    \n",
        "for file in cat_files[train_data_count:train_data_count+test_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(valid_cat_dir, file)\n",
        "    shutil.move(src, dst)\n",
        " \n",
        "for file in cat_files[train_data_count+test_data_count:total_data_count]:\n",
        "    src = os.path.join(data_cat_dir, file)\n",
        "    dst = os.path.join(test_cat_dir, file)"
      ],
      "metadata": {
        "id": "jfUgp1olqtW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti =deepdriver.Artifacts(name=\"cat_dog\",type=\"DATASET\")"
      ],
      "metadata": {
        "id": "OaxyQJYNqwSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arti.add(\"./cat_dog\")"
      ],
      "metadata": {
        "id": "n6wSWFGhq_07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deepdriver.upload_artifact(arti)"
      ],
      "metadata": {
        "id": "a2-Vkpu4qG8X",
        "outputId": "2c029a47-01c7-41c2-ad45-4baeb12342af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading: [./cat_dog/test_set/dog/dog.920.jpg] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100.0%] [1900/1900]"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IkvhPhjxSs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. ü•º create hpo"
      ],
      "metadata": {
        "id": "gWafO8QnRpc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name=\"cat_dog_cnn_hpo_with_artifact_exp4\""
      ],
      "metadata": {
        "id": "6dW_6K0Pq2xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hpo_configuration = {\n",
        "    \"metric\": {\"name\": \"accuracy\", \"goal\": \"maximize\"},\n",
        "    \"method\": \"grid\",\n",
        "    \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [10e-2,10e-3,10e-4,10e-5]\n",
        "        },\n",
        "        \"hidden_layer\":{\n",
        "            \"values\": [32,64,128]\n",
        "        },\n",
        "        \"epoch\" :{\n",
        "            \"values\": [10]\n",
        "        }\n",
        "\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Mm23Iq3NXa7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment init & config hyperparam\n",
        "deepdriver.create_hpo(exp_name= experiment_name,hpo_config= hpo_configuration)"
      ],
      "metadata": {
        "id": "z93NrRBUsvDm",
        "outputId": "ee9327a5-8647-4586-9571-b3c8e0a7c0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HPO initialized\n",
            "Team Name=molamola.python\n",
            "Exp Name=cat_dog_cnn_hpo_with_artifact_exp4\n",
            "Exp Url=/experi/molamola.python/cat_dog_cnn_hpo_with_artifact_exp4/exp/chart\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-01-27 00:35:22,944]\u001b[0m A new study created in RDB with name: molamola.python_cat_dog_cnn_hpo_with_artifact_exp4\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('success', 573)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. üìö train your code and send log"
      ],
      "metadata": {
        "id": "CNDciDDURuZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make model"
      ],
      "metadata": {
        "id": "wbFA68eYps1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artifact={ \n",
        "   \"dataset\":{\n",
        "       \"exp_name\": \"upload_data_code\",\n",
        "        \"type\" : \"DATASET\", \n",
        "        \"name\" : \"cat_dog\", \n",
        "   }, \n",
        "    \"code\" :\n",
        "    { \n",
        "        \"exp_name\": \"upload_data_code\",\n",
        "        \"type\" : \"CODE\", \n",
        "        \"name\" : \"cnn_image_3\", \n",
        "        \"file\": \"main.py\", \n",
        "        \"fun\": \"train\"\n",
        "   }\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "guYZyGrpuOQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " deepdriver.run_hpo(exp_name=experiment_name, artifact= artifact, count=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4GuWFXW1rHX",
        "outputId": "73642040-0a72-4db3-ecf5-152a15c6d1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artifact is got! \n",
            " artifact id :{441}\n",
            "Downloading: [./deepdriver/artifact/441/V3/valid_set/dog/dog.899.jpg] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100.0%] [1900/1900]artifact is got! \n",
            " artifact id :{492}\n",
            "Downloading: [./deepdriver/artifact/492/V1/main.py] |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| [100.0%] [1/1]DeepDriver initialized\n",
            "Team Name=molamola.python\n",
            "Exp Name=cat_dog_cnn_hpo_with_artifact_exp4\n",
            "Run Name=run-3\n",
            "Run URL=http://54.180.86.146:9111/experi/molamola.python/cat_dog_cnn_hpo_with_artifact_exp4/run-3/run/chart\n",
            "[('dataset_path', './deepdriver/artifact/441'), ('code_path', './deepdriver/artifact/492'), ('batch_size', 64), ('learning_rate', 0.01), ('hidden_layer', 32), ('epoch', 10)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m[W 2023-01-27 00:56:21,569]\u001b[0m Trial 2 failed with parameters: {'batch_size': 64, 'learning_rate': 0.01, 'hidden_layer': 32, 'epoch': 10} because of the following error: FileNotFoundError(2, 'No such file or directory').\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 215, in <lambda>\n",
            "    return lambda trial: objective(trial, exp_name, team_name, func, config)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 220, in objective\n",
            "    y = func()\n",
            "  File \"./deepdriver/artifact/492/V1/main.py\", line 55, in train\n",
            "    train_generator, validation_generator, test_generator = prepare_data(deepdriver.config.batch_size)\n",
            "  File \"./deepdriver/artifact/492/V1/main.py\", line 35, in prepare_data\n",
            "    train_generator = train_datagen.flow_from_directory(train_set_dir,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\", line 1469, in flow_from_directory\n",
            "    return DirectoryIterator(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\", line 507, in __init__\n",
            "    for subdir in sorted(os.listdir(directory)):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './deepdriver/artifact/441/train_set'\n",
            "\u001b[33m[W 2023-01-27 00:56:21,573]\u001b[0m Trial 2 failed with value None.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: './deepdriver/artifact/441/train_set'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 260, in run_optimize\n",
            "    study = study.optimize(get_wrapped_func(exp_name, team_name_, func, config), n_trials=count)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/study.py\", line 425, in optimize\n",
            "    _optimize(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 66, in _optimize\n",
            "    _optimize_sequential(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 163, in _optimize_sequential\n",
            "    frozen_trial = _run_trial(study, func, catch)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 251, in _run_trial\n",
            "    raise func_err\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 215, in <lambda>\n",
            "    return lambda trial: objective(trial, exp_name, team_name, func, config)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/deepdriver/sdk/experiment.py\", line 220, in objective\n",
            "    y = func()\n",
            "  File \"./deepdriver/artifact/492/V1/main.py\", line 55, in train\n",
            "    train_generator, validation_generator, test_generator = prepare_data(deepdriver.config.batch_size)\n",
            "  File \"./deepdriver/artifact/492/V1/main.py\", line 35, in prepare_data\n",
            "    train_generator = train_datagen.flow_from_directory(train_set_dir,\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\", line 1469, in flow_from_directory\n",
            "    return DirectoryIterator(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/preprocessing/image.py\", line 507, in __init__\n",
            "    for subdir in sorted(os.listdir(directory)):\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './deepdriver/artifact/441/train_set'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dn_UkcTT2kvY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}